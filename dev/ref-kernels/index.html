<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernels · GpABC</title><link rel="canonical" href="https://tanhevg.github.io/GpABC.jl/stable/ref-kernels/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GpABC</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Package Overview</span><ul><li><a class="toctext" href="../overview-abc/">ABC Parameter Inference</a></li><li><a class="toctext" href="../overview-ms/">ABC Model Selection</a></li><li><a class="toctext" href="../overview-lna/">LNA</a></li><li><a class="toctext" href="../overview-gp/">Gaussian Process Regression</a></li><li><a class="toctext" href="../summary_stats/">Summary Statistics</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../example-abc/">ABC Parameter Inference</a></li><li><a class="toctext" href="../example-ms/">ABC Model Selection</a></li><li><a class="toctext" href="../example-lna/">Stochastic inference (LNA)</a></li><li><a class="toctext" href="../example-gp/">Gaussian Processes</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../ref-abc/">ABC Basic</a></li><li><a class="toctext" href="../ref-abc-advanced/">ABC Advanced</a></li><li><a class="toctext" href="../ref-lna/">Stochastic inference (LNA)</a></li><li><a class="toctext" href="../ref-ms/">Model Selection</a></li><li><a class="toctext" href="../ref-gp/">Gaussian Processes</a></li><li class="current"><a class="toctext" href>Kernels</a><ul class="internal"><li><a class="toctext" href="#Index-1">Index</a></li><li><a class="toctext" href="#Types-and-Functions-1">Types and Functions</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Kernels</a></li></ul><a class="edit-page" href="https://github.com/tanhevg/GpABC.jl/blob/master/docs/src/ref-kernels.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Kernels</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Kernels-Reference-1" href="#Kernels-Reference-1">Kernels Reference</a></h1><p><code>GpABC</code> functions and types for working with kernels.</p><h2><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h2><ul><li><a href="#GpABC.AbstractGPKernel"><code>GpABC.AbstractGPKernel</code></a></li><li><a href="#GpABC.MaternArdKernel"><code>GpABC.MaternArdKernel</code></a></li><li><a href="#GpABC.MaternIsoKernel"><code>GpABC.MaternIsoKernel</code></a></li><li><a href="#GpABC.SquaredExponentialArdKernel"><code>GpABC.SquaredExponentialArdKernel</code></a></li><li><a href="#GpABC.SquaredExponentialIsoKernel"><code>GpABC.SquaredExponentialIsoKernel</code></a></li><li><a href="#GpABC.ExponentialArdKernel-Tuple{}"><code>GpABC.ExponentialArdKernel</code></a></li><li><a href="#GpABC.ExponentialIsoKernel-Tuple{}"><code>GpABC.ExponentialIsoKernel</code></a></li><li><a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.covariance</code></a></li><li><a href="#GpABC.covariance_diagonal-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpABC.covariance_diagonal</code></a></li><li><a href="#GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.covariance_grad</code></a></li><li><a href="#GpABC.covariance_training-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpABC.covariance_training</code></a></li><li><a href="#GpABC.get_hyperparameters_size-Tuple{AbstractGPKernel,AbstractArray{Float64,2}}"><code>GpABC.get_hyperparameters_size</code></a></li><li><a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.scaled_squared_distance</code></a></li><li><a href="#GpABC.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.scaled_squared_distance_grad</code></a></li></ul><h2><a class="nav-anchor" id="Types-and-Functions-1" href="#Types-and-Functions-1">Types and Functions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.AbstractGPKernel" href="#GpABC.AbstractGPKernel"><code>GpABC.AbstractGPKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">AbstractGPKernel</code></pre><p>Abstract kernel type. User-defined kernels should derive from it.</p><p>Implementations have to provide methods for <a href="#GpABC.get_hyperparameters_size-Tuple{AbstractGPKernel,AbstractArray{Float64,2}}"><code>get_hyperparameters_size</code></a> and <a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>. Methods for <a href="#GpABC.covariance_training-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_training</code></a>, <a href="#GpABC.covariance_diagonal-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_diagonal</code></a> and <a href="#GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a> are optional.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/abstract_kernel.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.covariance</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">covariance(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})</code></pre><p>Return the covariance matrix. Should be overridden by kernel implementations.</p><p><strong>Arguments</strong></p><ul><li><code>ker</code>: The kernel object. Implementations must override with their own subtype.</li><li><code>log_theta</code>: natural logarithm of hyperparameters.</li><li><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions <span>$n \times d$</span>; <code>z</code> must have dimensions <span>$m \times d$</span>.</li></ul><p><strong>Return</strong></p><p>The covariance matrix, of size <span>$n \times m$</span>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/abstract_kernel.jl#L12-L26">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.covariance_diagonal-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}" href="#GpABC.covariance_diagonal-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpABC.covariance_diagonal</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">covariance_diagonal(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2})</code></pre><p>This is a speedup version of <a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>, which is invoked if the caller is not interested in the entire covariance matrix, but only needs the variance, i.e. the diagonal of the covariance matrix.</p><p>Default method just returns <code>diag(covariance(...))</code>, with <code>x === z</code>. Kernel implementations can optionally override it to achieve betrer performance, by not computing the non diagonal elements of covariance matrix.</p><p>See <a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> for description of arguments.</p><p><strong>Return</strong></p><p>The 1-d array of variances, of size <code>size(x, 1)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/abstract_kernel.jl#L49-L65">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.covariance_grad</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">covariance_grad(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})</code></pre><p>Return the gradient of the covariance function with respect to logarigthms of hyperparameters, based on the provided direction matrix.</p><p>This function can be optionally overridden by kernel implementations. If the gradient function is not provided, <a href="../ref-gp/#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>gp_train</code></a> will fail back to <code>NelderMead</code> algorithm by default.</p><p><strong>Arguments</strong></p><ul><li><code>ker</code>: The kernel object. Implementations must override with their own subtype.</li><li><code>log_theta</code>:  natural logarithm of hyperparameters</li><li><code>x</code>: Training data, reshaped into a 2-d array. <code>x</code> must have dimensions <span>$n \times d$</span>.</li><li><code>R</code> the directional matrix, <span>$n \times n$</span></li></ul><div>\[R = \frac{1}{\sigma_n^2}(\alpha * \alpha^T - K^{-1}); \alpha = K^{-1}y\]</div><p><strong>Return</strong></p><p>A vector of size <code>length(log_theta)</code>, whose <span>$j$</span>&#39;th element is equal to</p><div>\[tr(R \frac{\partial K}{\partial \eta_j})\]</div></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/abstract_kernel.jl#L71-L97">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.covariance_training-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}" href="#GpABC.covariance_training-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpABC.covariance_training</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">covariance_training(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    training_x::AbstractArray{Float64, 2})</code></pre><p>This is a speedup version of <a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>, which is only called during traing sequence. Intermediate matrices computed in this function for particular hyperparameters can be cached and reused subsequently, either in this function or in <a href="#GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a></p><p>Default method just delegates to <a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> with <code>x === z</code>. Kernel implementations can optionally override it for betrer performance.</p><p>See <a href="#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> for description of arguments and return values.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/abstract_kernel.jl#L30-L43">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.get_hyperparameters_size-Tuple{AbstractGPKernel,AbstractArray{Float64,2}}" href="#GpABC.get_hyperparameters_size-Tuple{AbstractGPKernel,AbstractArray{Float64,2}}"><code>GpABC.get_hyperparameters_size</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">get_hyperparameters_size(kernel::AbstractGPKernel, training_data::AbstractArray{Float64, 2})</code></pre><p>Return the number of hyperparameters for used by this kernel on this training data set. Should be overridden by kernel implementations.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/abstract_kernel.jl#L103-L108">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.MaternArdKernel" href="#GpABC.MaternArdKernel"><code>GpABC.MaternArdKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MaternArdKernel &lt;: AbstractGPKernel</code></pre><p>Matérn kernel with distinct length scale for each dimention, <span>$l_k$</span>. Parameter <span>$\nu$</span> (nu) is passed in constructor. Currently, only values of <span>$\nu=1$</span>, <span>$\nu=3$</span> and <span>$\nu=5$</span> are supported.</p><div>\[\begin{aligned}
K_{\nu=1}(r) &amp;= \sigma_f^2e^{-\sqrt{r}}\\
K_{\nu=3}(r) &amp;= \sigma_f^2(1 + \sqrt{3r})e^{-\sqrt{3r}}\\
K_{\nu=5}(r) &amp;= \sigma_f^2(1 + \sqrt{3r} + \frac{5}{3}r)e^{-\sqrt{5r}}\\
r_{ij} &amp;= \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>The length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a <span>$d$</span>-dimensional space, this kernel needs <span>$d+1$</span> hyperparameters, in the following order:</p><ol><li><span>$\sigma_f$</span>: the signal standard deviation</li><li><span>$l_1, \ldots, l_d$</span>: the length scales for each dimension</li></ol></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/matern_kernels.jl#L54-L78">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.MaternIsoKernel" href="#GpABC.MaternIsoKernel"><code>GpABC.MaternIsoKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MaternIsoKernel &lt;: AbstractGPKernel</code></pre><p>Matérn kernel with uniform length scale across all dimensions, <span>$l$</span>. Parameter <span>$\nu$</span> (nu) is passed in constructor. Currently, only values of <span>$\nu=1$</span>, <span>$\nu=3$</span> and <span>$\nu=5$</span> are supported.</p><div>\[\begin{aligned}
K_{\nu=1}(r) &amp;= \sigma_f^2e^{-\sqrt{r}}\\
K_{\nu=3}(r) &amp;= \sigma_f^2(1 + \sqrt{3r})e^{-\sqrt{3r}}\\
K_{\nu=5}(r) &amp;= \sigma_f^2(1 + \sqrt{3r} + \frac{5}{3}r)e^{-\sqrt{5r}}\\
r_{ij} &amp;= \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>Hyperparameters vector for this kernel must contain two elements, in the following order:</p><ol><li><span>$\sigma_f$</span>: the signal standard deviation</li><li><span>$l$</span>: the length scale</li></ol></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/matern_kernels.jl#L8-L30">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.SquaredExponentialArdKernel" href="#GpABC.SquaredExponentialArdKernel"><code>GpABC.SquaredExponentialArdKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SquaredExponentialArdKernel &lt;: AbstractGPKernel</code></pre><p>Squared exponential kernel with distinct length scale for each dimention, <span>$l_k$</span>.</p><div>\[\begin{aligned}
K(r) &amp; = \sigma_f^2 e^{-r/2} \\
r_{ij} &amp; = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>The length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a <span>$d$</span>-dimensional space, this kernel needs <span>$d+1$</span> hyperparameters, in the following order:</p><ol><li><span>$\sigma_f$</span>: the signal standard deviation</li><li><span>$l_1, \ldots, l_d$</span>: the length scales for each dimension</li></ol></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/rbf_kernels.jl#L33-L52">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.SquaredExponentialIsoKernel" href="#GpABC.SquaredExponentialIsoKernel"><code>GpABC.SquaredExponentialIsoKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SquaredExponentialIsoKernel &lt;: AbstractGPKernel</code></pre><p>Squared exponential kernel with uniform length scale across all dimensions, <span>$l$</span>.</p><div>\[\begin{aligned}
K(r) &amp; = \sigma_f^2 e^{-r/2} \\
r_{ij} &amp; = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>Hyperparameters vector for this kernel must contain two elements, in the following order:</p><ol><li><span>$\sigma_f$</span>: the signal standard deviation</li><li><span>$l$</span>: the length scale</li></ol></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/rbf_kernels.jl#L7-L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.ExponentialArdKernel-Tuple{}" href="#GpABC.ExponentialArdKernel-Tuple{}"><code>GpABC.ExponentialArdKernel</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">ExponentialArdKernel</code></pre><p>Alias for <a href="#GpABC.MaternArdKernel"><code>MaternArdKernel</code></a>(1)</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/matern_kernels.jl#L84-L88">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.ExponentialIsoKernel-Tuple{}" href="#GpABC.ExponentialIsoKernel-Tuple{}"><code>GpABC.ExponentialIsoKernel</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">ExponentialIsoKernel</code></pre><p>Alias for <a href="#GpABC.MaternIsoKernel"><code>MaternIsoKernel</code></a>(1)</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/matern_kernels.jl#L47-L51">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.scaled_squared_distance</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">scaled_squared_distance(log_ell::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})</code></pre><p>Compute the scaled squared distance between <code>x</code> and <code>z</code>:</p><div>\[r_{ij} = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}\]</div><p>The gradient of this function with respect to length scale hyperparameter(s) is returned by <a href="#GpABC.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance_grad</code></a>.</p><p><strong>Arguments</strong></p><ul><li><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions <span>$n \times d$</span>; <code>z</code> must have dimensions <span>$m \times d$</span>.</li><li><code>log_ell</code>: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size <code>d</code> (ARD)</li></ul><p><strong>Return</strong></p><p>An <span>$n \times m$</span> matrix of scaled squared distances</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/scaled_squared_distance.jl#L1-L20">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpABC.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpABC.scaled_squared_distance_grad</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">scaled_squared_distance_grad(log_ell::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})</code></pre><p>Return the gradient of the <a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> function with respect to logarigthms of length scales, based on the provided direction matrix.</p><p><strong>Arguments</strong></p><ul><li><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions <span>$n \times d$</span>; <code>z</code> must have dimensions <span>$m \times d$</span>.</li><li><code>log_ell</code>: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size <code>d</code> (ARD)</li><li><code>R</code> the direction matrix, <span>$n \times m$</span>. This can be used to compute the gradient of a function that depends on <a href="#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> via the chain rule.</li></ul><p><strong>Return</strong></p><p>A vector of size <code>length(log_ell)</code>, whose <span>$k$</span>&#39;th element is equal to</p><div>\[\text{tr}(R \frac{\partial K}{\partial l_k})\]</div></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/26d00abee930903f454056a9f5c44e712a4eddaa/src/gp/kernels/scaled_squared_distance.jl#L45-L65">source</a></section><footer><hr/><a class="previous" href="../ref-gp/"><span class="direction">Previous</span><span class="title">Gaussian Processes</span></a></footer></article></body></html>
