<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian Process Regression · GpABC</title><link rel="canonical" href="https://tanhevg.github.io/GpABC.jl/stable/overview-gp/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GpABC</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Package Overview</span><ul><li><a class="toctext" href="../overview-abc/">ABC Parameter Inference</a></li><li><a class="toctext" href="../overview-ms/">ABC Model Selection</a></li><li><a class="toctext" href="../overview-lna/">LNA</a></li><li class="current"><a class="toctext" href>Gaussian Process Regression</a><ul class="internal"><li><a class="toctext" href="#Gaussian-Process,-Prior-and-Posterior-1">Gaussian Process, Prior and Posterior</a></li><li><a class="toctext" href="#Kernels-and-Hyperparameters-1">Kernels and Hyperparameters</a></li><li><a class="toctext" href="#Custom-Kernels-1">Custom Kernels</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li><li><a class="toctext" href="../summary_stats/">Summary Statistics</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../example-abc/">ABC Parameter Inference</a></li><li><a class="toctext" href="../example-ms/">ABC Model Selection</a></li><li><a class="toctext" href="../example-lna/">Stochastic inference (LNA)</a></li><li><a class="toctext" href="../example-gp/">Gaussian Processes</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../ref-abc/">ABC Basic</a></li><li><a class="toctext" href="../ref-abc-advanced/">ABC Advanced</a></li><li><a class="toctext" href="../ref-lna/">Stochastic inference (LNA)</a></li><li><a class="toctext" href="../ref-ms/">Model Selection</a></li><li><a class="toctext" href="../ref-gp/">Gaussian Processes</a></li><li><a class="toctext" href="../ref-kernels/">Kernels</a></li></ul></li><li><a class="toctext" href="../faq/">FAQ</a></li></ul></nav><article id="docs"><header><nav><ul><li>Package Overview</li><li><a href>Gaussian Process Regression</a></li></ul><a class="edit-page" href="https://github.com/tanhevg/GpABC.jl/blob/master/docs/src/overview-gp.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Gaussian Process Regression</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="gp-overview-1" href="#gp-overview-1">Gaussian Processes Regression Overview</a></h1><p>An excellent introduction to Gaussian Processes is available in the book by (Rasmussen and Williams, 2006).</p><ul><li><a href="#gp-overview-1">Gaussian Processes Regression Overview</a></li><ul><li><a href="#Gaussian-Process,-Prior-and-Posterior-1">Gaussian Process, Prior and Posterior</a></li><li><a href="#Kernels-and-Hyperparameters-1">Kernels and Hyperparameters</a></li><li><a href="#Custom-Kernels-1">Custom Kernels</a></li><li><a href="#References-1">References</a></li></ul></ul><h2><a class="nav-anchor" id="Gaussian-Process,-Prior-and-Posterior-1" href="#Gaussian-Process,-Prior-and-Posterior-1">Gaussian Process, Prior and Posterior</a></h2><p>A <em>Gaussian Process</em> (<strong>GP</strong>) is a collection of random variables, any finite number of which have a joint Gaussian distribution. This assumption is often referred to as the <em>GP prior</em>. In a regression setting, we are going to use GPs to approximate an unknown function <span>$f(x)$</span>, <span>$x$</span> being a <span>$d$</span>-dimensional feature vector, <span>$x \in \mathbb{R}^d$</span>. We assume that our training data set contains of <span>$n$</span> points in <span>$\mathbb{R}^d$</span>, and the test set - of <span>$m$</span> points in <span>$\mathbb{R}^d$</span>. We denote the training data set as <span>$\mathbf{x}, \mathbf{x} \in \mathbb{R}^{n \times d}$</span> and the test data set as <span>$\mathbf{x^*}, \mathbf{x^*} \in \mathbb{R}^{m \times d}$</span>. Function values on training and test data sets are denoted as <span>$\mathbf{y} = \mathbf{f(x)}$</span>, and <span>$\mathbf{y^*} = \mathbf{f(x^*)}$</span>, respectively (in vectorised form). We also assume that the mean of the prior Gaussian distribution is zero, and its covariance matrix is known. Furthermore, we split the covariance matrix into the following regions:</p><ul><li><span>$K$</span>: the covariance matrix computed on the training data, <span>$K \in \mathbb{R}^{n \times n}$</span></li><li><span>$K^{**}$</span>: the covariance matrix computed on the test data, <span>$K^{**} \in \mathbb{R}^{m \times m}$</span></li><li><span>$K^{*}$</span>: the covariance matrix between the training and test data, <span>$K^{*} \in \mathbb{R}^{n \times m}$</span></li></ul><p>In this notation, the GP prior can be written as</p><div>\[\left[ \begin{matrix}
\mathbf{y}\\
\mathbf{y^*}
\end{matrix} \right]
\sim \mathcal{N} \left( 0,
\left[ \begin{matrix}
K &amp; K^*\\
K^{*\top} &amp; K^{**}
\end{matrix} \right] \right)\]</div><p>The desired approximation of <span>$f$</span> in <span>$\mathbf{x^*}$</span> is the conditional distribution of <span>$\mathbf{y^*}$</span>, given <span>$\mathbf{x}$</span>, <span>$\mathbf{y}$</span> and <span>$\mathbf{x^*}$</span>. This distrubution, referred to as <em>GP posterior</em>, can be derived from the GP prior and the properties of a multivariate Normal distribution:</p><div>\[\begin{align*}
\mathbf{y^* | x, y, x^*} &amp; \sim \mathcal{N}(\mathbf{\tilde{y}}, \tilde{K}) \\
\mathbf{\tilde{y}} &amp; = K^{*\top} K^{-1} \mathbf{y} \\
\tilde{K} &amp; = K^{**} - K^{*\top} K^{-1} K^*
\end{align*}\]</div><p><span>$\mathbf{\tilde{y}}$</span> and <span>$\mathbf{\tilde{K}}$</span> are, respectively, the mean vector and the covariance matrix of the GP posterior. Often, we are not interested in non-diagonal elements of <span>$\mathbf{\tilde{K}}$</span>. In such cases just the vector of diagonal elements is reported.</p><h2><a class="nav-anchor" id="Kernels-and-Hyperparameters-1" href="#Kernels-and-Hyperparameters-1">Kernels and Hyperparameters</a></h2><p>We assume that the covariance between any two points <span>$x$</span> and <span>$x^{&#39;}$</span> is given by a <em>kernel function</em> <span>$k(x, x^{&#39;})$</span>, or in matrix notation, <span>$K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$</span>. This kernel function is parameterised by a vector of <em>hyperparameters</em> <span>$\mathbf{\eta} = \eta_1, \ldots, \eta_p$</span>. The covariance matrix is thus also dependent on <span>$\mathbf{\eta}$</span>: <span>$K = K(\mathbf{\eta})$</span>.</p><p>The optimal values of hyperparameters <span>$\hat{\eta}$</span> can be obtained by finding the maximum value of log likelihood of the GP prior:</p><div>\[\begin{align*}
\log p(\mathbf{y|\eta}) &amp;= -\frac{1}{2}\mathbf{y}^\top K^{-1} \mathbf{y} - \frac{1}{2}|K| - \frac{n}{2}\log(2\pi) \\
\hat{\eta} &amp;= \underset{\mathbf{\eta}}{\text{argmax}}(\log p(\mathbf{y}|\mathbf{\eta}))
\end{align*}\]</div><p>In <code>GpABC</code> this optimisation is performed using <a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim</code></a> package. By default, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/cg/">Conjugate Gradient</a> bounded box optimisation is used, as long as the gradient with respect to hyperparameters is implemented for the kernel function. If the gradient implementation is not provided, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/">Nelder Mead</a> optimiser is used by default.</p><p>The starting point of the optimisation can be specified by calling <a href="../ref-gp/#GpABC.set_hyperparameters-Tuple{GPModel,AbstractArray{Float64,1}}"><code>set_hyperparameters</code></a>. If the starting point has not been provided, optimisation will start from all hyperparameters set to 1. Default upper and lower bounds are set to <span>$e^{10}$</span> and <span>$e^{−10}$</span> , respectively, for each hyperparameter.</p><p>For numerical stability the package uses logarithms of hyperparameters internally, when calling the log likelihood and kernel functions. Logarithmisation and exponentiation back takes place in <a href="../ref-gp/#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>gp_train</code></a> function.</p><p>The log likelihood function with log hyperparameters is implemented by <a href="../ref-gp/#GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood_log</code></a>. This is the target function of the optimisation procedure in <a href="../ref-gp/#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>gp_train</code></a>. There is also a version of log likelihood with actual (non-log) hyperparameters: <a href="../ref-gp/#GpABC.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood</code></a>. The gradient of the log likelihood function with respect to logged hyperparameters is implemented by <a href="../ref-gp/#GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood_grad</code></a>.</p><p>Depending on the kernel, it is not uncommon for the log likelihood function to have multiple local optima. If a trained GP produces an unsatisfactory data fit, one possible workaround is trying to run <a href="../ref-gp/#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>gp_train</code></a> several times with random starting points.</p><p><code>Optim</code> has a built in constraint of running no more than 1000 iterations of any optimisation algorithm. <code>GpABC</code> relies on this feature to ensure that the training procedure does not get stuck forever. As a consequence, the optimizer might exit prematurely, before reaching the local optimum. Setting <code>log_level</code> argument of <a href="../ref-gp/#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>gp_train</code></a> to a value greater than zero will make it log its actions to standard output, including whether the local minimum has been reached or not.</p><p>It is often convenient to model the measurement noise in the training data separately. This amounts to a normally distributed random variable being added to <span>$\mathbf{y}$</span>. Denoting the variance of this random noise as <span>$\sigma_n$</span>, this is equivalent to altering the covariance matrix to <span>$K_y = K + \sigma_n I$</span>, where <span>$I$</span> is the identity matrix. Noise variance <span>$\sigma_n$</span> is also a hyperparameter, that must be optimised with the rest of kernel hyperparameters. <code>GpABC</code> uses a joint hyperparameter vector, where <span>$\sigma_n$</span> is always the last element.</p><h2><a class="nav-anchor" id="Custom-Kernels-1" href="#Custom-Kernels-1">Custom Kernels</a></h2><p><code>GpABC</code> ships with an extensible library of kernel functions. Each kernel is represented with a type that derives from <a href="../ref-kernels/#GpABC.AbstractGPKernel"><code>AbstractGPKernel</code></a>:</p><ul><li><a href="../ref-kernels/#GpABC.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a></li><li><a href="../ref-kernels/#GpABC.SquaredExponentialArdKernel"><code>SquaredExponentialArdKernel</code></a></li><li><a href="../ref-kernels/#GpABC.MaternIsoKernel"><code>MaternIsoKernel</code></a></li><li><a href="../ref-kernels/#GpABC.MaternArdKernel"><code>MaternArdKernel</code></a></li><li><a href="../ref-kernels/#GpABC.ExponentialIsoKernel-Tuple{}"><code>ExponentialIsoKernel</code></a></li><li><a href="../ref-kernels/#GpABC.ExponentialArdKernel-Tuple{}"><code>ExponentialArdKernel</code></a></li></ul><p>These kernels rely on matrix of scaled squared distances between training/test inputs <span>$[r_{ij}]$</span>, which is computed by <a href="../ref-kernels/#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> function. The gradient vector of scaled squared distance derivatives with respect to length scale hyperparameter(s) is returned by <a href="../ref-kernels/#GpABC.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance_grad</code></a> function.</p><p>The kernel covariance matrix is returned by function <a href="../ref-kernels/#GpABC.covariance-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>. Optional speedups of this function <a href="../ref-kernels/#GpABC.covariance_diagonal-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_diagonal</code></a> and <a href="../ref-kernels/#GpABC.covariance_training-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_training</code></a> are implemented for the pre-shipped kernels. The gradient with respect to log hyperparameters is computed by <a href="../ref-kernels/#GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a>. The <code>log_theta</code> argument refers to the logarithms of kernel hyperparameters. Note that hyperparameters that do not affect the kernel (e.g. <span>$\sigma_n$</span> ) are not included in <code>log_theta</code>.</p><p>Custom kernels functions can be implemented  by adding more types that inherit from <a href="../ref-kernels/#GpABC.AbstractGPKernel"><code>AbstractGPKernel</code></a>.</p><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><ul><li>Rasmussen, C. E., &amp; Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. ISBN 0-262-18253-X. <a href="http://www.gaussianprocess.org/gpml">http://www.gaussianprocess.org/gpml</a></li></ul><footer><hr/><a class="previous" href="../overview-lna/"><span class="direction">Previous</span><span class="title">LNA</span></a><a class="next" href="../summary_stats/"><span class="direction">Next</span><span class="title">Summary Statistics</span></a></footer></article></body></html>
