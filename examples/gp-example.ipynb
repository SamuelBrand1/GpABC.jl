{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression with `GpABC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling GpABC [e850a1a4-d859-11e8-3d54-a195e6d045d3]\n",
      "└ @ Base loading.jl:1192\n",
      "┌ Warning: Module Optim with build ID 613797266333516 is missing from the cache.\n",
      "│ This may mean Optim [429524aa-4258-5aef-a3af-852621145aeb] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n",
      "┌ Info: Recompiling stale cache file /home/tah17/.julia/compiled/v1.0/Distributions/xILW0.ji for Distributions [31c24e10-a181-5473-b8eb-7969acd0382f]\n",
      "└ @ Base loading.jl:1190\n",
      "┌ Warning: Module StatsBase with build ID 1139040239825804 is missing from the cache.\n",
      "│ This may mean StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n",
      "┌ Info: Recompiling stale cache file /home/tah17/.julia/compiled/v1.0/StatsFuns/530lR.ji for StatsFuns [4c63d2b9-4356-54db-8cca-17b64c39e42c]\n",
      "└ @ Base loading.jl:1190\n",
      "┌ Warning: Module SpecialFunctions with build ID 249404284239831 is missing from the cache.\n",
      "│ This may mean SpecialFunctions [276daf66-3868-5448-9aa4-cd146d93841b] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: LoadError: LoadError: UndefVarError: loggamma not defined\nin expression starting at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/misc.jl:60\nin expression starting at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/StatsFuns.jl:238\nin expression starting at /home/tah17/.julia/packages/Distributions/OdJGZ/src/Distributions.jl:3",
     "output_type": "error",
     "traceback": [
      "LoadError: LoadError: LoadError: UndefVarError: loggamma not defined\nin expression starting at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/misc.jl:60\nin expression starting at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/StatsFuns.jl:238\nin expression starting at /home/tah17/.julia/packages/Distributions/OdJGZ/src/Distributions.jl:3",
      "",
      "Stacktrace:",
      " [1] lstirling_asym(::BigFloat) at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/misc.jl:56",
      " [2] lstirling_asym(::BigInt) at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/misc.jl:58",
      " [3] top-level scope at none:0",
      " [4] include at ./boot.jl:317 [inlined]",
      " [5] include_relative(::Module, ::String) at ./loading.jl:1044",
      " [6] include at ./sysimg.jl:29 [inlined]",
      " [7] include(::String) at /home/tah17/.julia/packages/StatsFuns/r47Mk/src/StatsFuns.jl:3",
      " [8] top-level scope at none:0",
      " [9] include at ./boot.jl:317 [inlined]",
      " [10] include_relative(::Module, ::String) at ./loading.jl:1044",
      " [11] _require(::Base.PkgId) at ./loading.jl:986",
      " [12] require(::Base.PkgId) at ./loading.jl:858",
      " [13] require(::Module, ::Symbol) at ./loading.jl:853",
      " [14] include at ./boot.jl:317 [inlined]",
      " [15] include_relative(::Module, ::String) at ./loading.jl:1044",
      " [16] _require(::Base.PkgId) at ./loading.jl:986",
      " [17] require(::Base.PkgId) at ./loading.jl:858",
      " [18] require(::Module, ::Symbol) at ./loading.jl:853",
      " [19] include at ./boot.jl:317 [inlined]",
      " [20] include_relative(::Module, ::String) at ./loading.jl:1044",
      " [21] _require(::Base.PkgId) at ./loading.jl:986",
      " [22] require(::Base.PkgId) at ./loading.jl:858",
      " [23] require(::Module, ::Symbol) at ./loading.jl:853",
      " [24] top-level scope at In[13]:1"
     ]
    }
   ],
   "source": [
    "using GpABC, Distributions, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Define the latent function that we are going to approximate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = x ^ 2 + 10 * sin(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some training and test data. Random noise is added to observations in training points, to make the task a little bit harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: Uniform not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Uniform not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[15]:2"
     ]
    }
   ],
   "source": [
    "n = 30\n",
    "training_x = sort(rand(Uniform(-10, 10), n))\n",
    "training_y = f.(training_x)\n",
    "training_y += 20 * (rand(n) .- 0.5) # add some noise\n",
    "test_x = range(min(training_x...), stop=max(training_x...), length=1000) |> collect;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known hyperparameters\n",
    "\n",
    "The package is built around a type `GPModel`, which encapsulates all the information\n",
    "required for training the Gaussian Process and performing the regression. In the simplest\n",
    "scenario the user would instantiate this type with some training data and labels, provide\n",
    "the hyperparameters and run the regression. `SquaredExponentialIsoKernel` will be used by default. \n",
    "\n",
    "Assume we already know the kernel hyperparameters:\n",
    "\n",
    "- $\\sigma_f = 37.08$\n",
    "- $l = 1.0 $\n",
    "- $\\sigma_n = 6.58$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = [37.08, 1.0, 6.58];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the regression and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: GPModel not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: GPModel not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[17]:1"
     ]
    }
   ],
   "source": [
    "gpm = GPModel(training_x, training_y)\n",
    "set_hyperparameters(gpm, hypers)\n",
    "test_y, test_var = gp_regression(test_x, gpm)\n",
    "\n",
    "plot(test_x, test_y, ribbon=1.96 * sqrt.(test_var), c=:red, linewidth=2, label=\"Approximation\")\n",
    "plot!(test_x, f.(test_x), c=:blue, linewidth=2, label=\"True function\")\n",
    "scatter!(training_x, training_y, m=:star4, label=\"Noisy training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the hyperparameters\n",
    "\n",
    "Normally, kernel hyperparameters are not known in advance. In this scenario the `gp_train` function should be used to find the Maximum Likelihood Estimate (MLE) of hyperparameters. By default,\n",
    "[Conjugate Gradient](http://julianlsolvers.github.io/Optim.jl/stable/algo/cg/) bounded box optimisation is used, as long as the gradient\n",
    "with respect to hyperparameters is implemented for the kernel function. If the gradient\n",
    "implementation is not provided, [Nelder Mead](http://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/) optimiser is used by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: gp_train not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: gp_train not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[18]:1"
     ]
    }
   ],
   "source": [
    "gp_train(gpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the regression with optimised hyperparameters, and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: gp_regression not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: gp_regression not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[19]:1"
     ]
    }
   ],
   "source": [
    "test_y, test_var = gp_regression(test_x, gpm)\n",
    "\n",
    "plot(test_x, test_y, ribbon=1.96 * sqrt.(test_var), c=:red, linewidth=2, label=\"Approximation\")\n",
    "plot!(test_x, f.(test_x), c=:blue, linewidth=2, label=\"True function\")\n",
    "scatter!(training_x, training_y, m=:star4, label=\"Noisy training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced usage of `gp_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: GPModel not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: GPModel not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[20]:2"
     ]
    }
   ],
   "source": [
    "import Optim.SimulatedAnnealing\n",
    "gpm = GPModel(training_x, training_y)\n",
    "gp_train(gpm; optimiser=SimulatedAnnealing(), \n",
    "    hp_lower=exp.([-10.0, -1.0, -10.0]), \n",
    "    hp_upper=exp.([10.0, 2.0, 10.0]), \n",
    "    log_level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a cutsom kernel\n",
    "\n",
    "Suppose we want to implement our own kernel function that adds a periodic element to the standard SE ISO kernel:\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2l^2}\\right) + \\exp(-2\\sin^2(\\sigma_g\\pi(x - x')))\n",
    "$$\n",
    "\n",
    "This kernel introduces a new hyperparameter, $\\sigma_g$, in addition to the standard hyperparameters of $\\sigma_f$ and $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: could not import GpABC.covariance into Main\n",
      "WARNING: could not import GpABC.get_hyperparameters_size into Main\n",
      "WARNING: could not import GpABC.covariance_grad into Main\n",
      "WARNING: could not import GpABC.covariance_training into Main\n",
      "WARNING: could not import GpABC.covariance_diagonal into Main\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: AbstractGPKernel not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: AbstractGPKernel not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at none:0"
     ]
    }
   ],
   "source": [
    "import GpABC.covariance, GpABC.get_hyperparameters_size, \n",
    "GpABC.covariance_grad, GpABC.covariance_training, GpABC.covariance_diagonal\n",
    "\n",
    "mutable struct SeIsoPeriodicKernelCache\n",
    "    last_theta::AbstractArray{Float64, 1}\n",
    "    D2::AbstractArray{Float64, 2} \n",
    "    D::AbstractArray{Float64, 2}\n",
    "    se_part::AbstractArray{Float64, 2} \n",
    "    periodic_part::AbstractArray{Float64, 2}\n",
    "end\n",
    "  \n",
    "SeIsoPeriodicKernelCache() = SeIsoPeriodicKernelCache(zeros(0), zeros(0, 0), zeros(0, 0), zeros(0, 0), zeros(0, 0))\n",
    "\n",
    "struct SeIsoPeriodicKernel <: AbstractGPKernel\n",
    "    cache::SeIsoPeriodicKernelCache\n",
    "end\n",
    "SeIsoPeriodicKernel() = SeIsoPeriodicKernel(SeIsoPeriodicKernelCache())\n",
    "\n",
    "function get_hyperparameters_size(ker::SeIsoPeriodicKernel, training_data::AbstractArray{Float64, 2})\n",
    "    3\n",
    "end\n",
    "\n",
    "function covariance(ker::SeIsoPeriodicKernel, log_theta::AbstractArray{Float64, 1}, \n",
    "        x1::AbstractArray{Float64, 2}, x2::AbstractArray{Float64, 2})\n",
    "    D2 = scaled_squared_distance([log_theta[2]], x1, x2)\n",
    "    n = size(x1, 1)\n",
    "    m = size(x2, 1)\n",
    "    D = repeat(x1, 1, m) - repeat(x2', n, 1)\n",
    "    sigma_f = exp(log_theta[1] * 2)\n",
    "    sigma_g = exp(log_theta[3])\n",
    "    K = sigma_f .* exp.(-D2 ./ 2) .+ exp.(-2 .* (sin.(pi * sigma_g .* D).*sin.(pi * sigma_g .* D)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that defining the kernel gradient with respect to hyperparameters is optional, and we are skipping it here. This means that it will not be possible to use gradient-based optimisation for GP training, and Nelder-Mead algorithm will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: SeIsoPeriodicKernel not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: SeIsoPeriodicKernel not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[22]:1"
     ]
    }
   ],
   "source": [
    "gpm = GPModel(training_x, training_y, SeIsoPeriodicKernel())\n",
    "gp_train(gpm; log_level=1)\n",
    "test_y, test_var = gp_regression(test_x, gpm)\n",
    "\n",
    "plot(test_x, test_y, ribbon=1.96 * sqrt.(test_var), c=:red, linewidth=2, label=\"Approximation\")\n",
    "plot!(test_x, f.(test_x), c=:blue, linewidth=2, label=\"True function\")\n",
    "scatter!(training_x, training_y, m=:star4, label=\"Noisy training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the gradient for the new kernel, as well as short cirquit functions for computing the covariance using cached computation results (`covariance_training`), and for diagonal-only variance (`covariance_diagonal`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: missing comma or ) in argument list",
     "output_type": "error",
     "traceback": [
      "syntax: missing comma or ) in argument list",
      ""
     ]
    }
   ],
   "source": [
    "function update_cache(cache::SeIsoPeriodicKernelCache, log_theta::AbstractArray{Float64, 1}, x::AbstractArray{Float64, 2})\n",
    "    sigma_f = exp(log_theta[1] * 2)\n",
    "    sigma_g = exp(log_theta[3])\n",
    "    D2 = scaled_squared_distance([log_theta[2]], x, x)\n",
    "    n = size(x, 1)\n",
    "    D = repeat(x, 1, n) - repeat(x', n, 1)\n",
    "    cache.last_theta = copy(log_theta)\n",
    "    cache.se_part = sigma_f .* exp.(-D2 ./ 2)\n",
    "    cache.periodic_part = exp.(-2 .* (sin.(pi * sigma_g .* D).^2)\n",
    "    cache.D2 = D2\n",
    "    cache.D = D\n",
    "    0\n",
    "end\n",
    "\n",
    "function covariance_grad(ker::SeIsoPeriodicKernel, log_theta::AbstractArray{Float64, 1}, \n",
    "        x::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2}) \n",
    "    cache = ker.cache\n",
    "    if log_theta != cache.last_theta \n",
    "        update_cache(ker.cache, log_theta, x)\n",
    "    end\n",
    "    KR = cache.se_part .* R\n",
    "    d1 = 2 * sum(KR)\n",
    "    d2 = KR[:]' * cache.D2[:]\n",
    "    sigma_g = exp(log_theta[3])\n",
    "    periodic_arg = sigma_g .* pi .* cache.D\n",
    "    d3 = sum(R' .* cache.periodic_part .* -2 .* sin.(2 .* periodic_arg) .* periodic_arg)\n",
    "    return [d1, d2, d3]\n",
    "end\n",
    "\n",
    "function covariance_training(ker::SeIsoPeriodicKernel,\n",
    "        log_theta::AbstractArray{Float64, 1}, x::AbstractArray{Float64, 2}) \n",
    "    if log_theta != ker.cache.last_theta\n",
    "        update_cache(ker.cache, log_theta, x)\n",
    "    end\n",
    "    return ker.cache.periodic_part + ker.cache.se_part \n",
    "end\n",
    "  \n",
    "function covariance_diagonal(ker::SeIsoPeriodicKernel, log_theta::AbstractArray{Float64, 1}, x::AbstractArray{Float64, 2})\n",
    "    fill(exp(log_theta[1] * 2) + 1, (size(x, 1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train the GP and run the regression, we can see that Conjugate Gradient Descent is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: SeIsoPeriodicKernel not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: SeIsoPeriodicKernel not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[24]:1"
     ]
    }
   ],
   "source": [
    "gpm = GPModel(training_x, training_y, SeIsoPeriodicKernel())\n",
    "gp_train(gpm; log_level=1)\n",
    "test_y, test_var = gp_regression(test_x, gpm)\n",
    "\n",
    "plot(test_x, test_y, ribbon=1.96 * sqrt.(test_var), c=:red, linewidth=2, label=\"Approximation\")\n",
    "plot!(test_x, f.(test_x), c=:blue, linewidth=2, label=\"True function\")\n",
    "scatter!(training_x, training_y, m=:star4, label=\"Noisy training data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
